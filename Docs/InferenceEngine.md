# Deep Learning Inference Engine with CUDA

- **Objective**: Build a lightweight inference engine for CNNs or Transformers, optimized using CUDA kernels for matrix operations and convolutions.
- **Focus Areas**:
  - Custom CUDA kernels for GEMM, convolution, and activation functions.
  - Memory optimization (shared, constant, global).
  - Integration with ONNX models.